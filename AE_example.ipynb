{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzC-ArZn6F7a"
   },
   "outputs": [],
   "source": [
    "# link drive https://drive.google.com/drive/folders/18-MEX-KK80-NhISMBUj1K-AsJyr-EuX_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ONNhpRO7YZ8o"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "qzmwPgoXZGm2"
   },
   "outputs": [],
   "source": [
    "from Model import Encoder, Decoder, AE_model\n",
    "from Train import AE_train\n",
    "from LOSS import AE_loss_function\n",
    "from sklearnEval import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "lJY56oNGYsPO"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "with open('X_Y.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "input = torch.from_numpy(b['data'].astype('float'))\n",
    "test = input\n",
    "input = input.float()\n",
    "\n",
    "target = b['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "GLPNvgeJY2L-"
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")\n",
    "batch_size = 1\n",
    "x_dim  = 500\n",
    "hidden_dim = 300\n",
    "latent_dim = 15\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "if3txepXK_gu"
   },
   "source": [
    "### Step 2. Define our model: Variational AutoEncoder (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "YP5D91BLK_gv"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n",
    "model = AE_model(Encoder=encoder, Decoder=decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zupXwYMEK_gw"
   },
   "source": [
    "### Step 3. Define Loss function (reprod. loss) and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "h9DOjmpEK_gw"
   },
   "outputs": [],
   "source": [
    "loss_function = AE_loss_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUfzB9PUamCj"
   },
   "source": [
    "## Step 4: Train VAE **model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "EDPFHwTyakDs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training VAE...\n",
      "\tEpoch 1 complete! \tAverage Loss:  0.277831107378006\n",
      "\tEpoch 2 complete! \tAverage Loss:  0.27065131068229675\n",
      "\tEpoch 3 complete! \tAverage Loss:  0.26152628660202026\n",
      "\tEpoch 4 complete! \tAverage Loss:  0.2509188950061798\n",
      "\tEpoch 5 complete! \tAverage Loss:  0.2394275814294815\n",
      "\tEpoch 6 complete! \tAverage Loss:  0.22770805656909943\n",
      "\tEpoch 7 complete! \tAverage Loss:  0.2164417803287506\n",
      "\tEpoch 8 complete! \tAverage Loss:  0.20618900656700134\n",
      "\tEpoch 9 complete! \tAverage Loss:  0.19703780114650726\n",
      "\tEpoch 10 complete! \tAverage Loss:  0.18839876353740692\n",
      "\tEpoch 11 complete! \tAverage Loss:  0.17947252094745636\n",
      "\tEpoch 12 complete! \tAverage Loss:  0.17039361596107483\n",
      "\tEpoch 13 complete! \tAverage Loss:  0.16196048259735107\n",
      "\tEpoch 14 complete! \tAverage Loss:  0.15476514399051666\n",
      "\tEpoch 15 complete! \tAverage Loss:  0.14892631769180298\n",
      "\tEpoch 16 complete! \tAverage Loss:  0.14415787160396576\n",
      "\tEpoch 17 complete! \tAverage Loss:  0.14009134471416473\n",
      "\tEpoch 18 complete! \tAverage Loss:  0.13645553588867188\n",
      "\tEpoch 19 complete! \tAverage Loss:  0.13299010694026947\n",
      "\tEpoch 20 complete! \tAverage Loss:  0.1296050250530243\n",
      "\tEpoch 21 complete! \tAverage Loss:  0.12625762820243835\n",
      "\tEpoch 22 complete! \tAverage Loss:  0.122963547706604\n",
      "\tEpoch 23 complete! \tAverage Loss:  0.11967015266418457\n",
      "\tEpoch 24 complete! \tAverage Loss:  0.1163676530122757\n",
      "\tEpoch 25 complete! \tAverage Loss:  0.11302035301923752\n",
      "\tEpoch 26 complete! \tAverage Loss:  0.10957653075456619\n",
      "\tEpoch 27 complete! \tAverage Loss:  0.1061098724603653\n",
      "\tEpoch 28 complete! \tAverage Loss:  0.10269976407289505\n",
      "\tEpoch 29 complete! \tAverage Loss:  0.09944453835487366\n",
      "\tEpoch 30 complete! \tAverage Loss:  0.09643122553825378\n",
      "\tEpoch 31 complete! \tAverage Loss:  0.0937269777059555\n",
      "\tEpoch 32 complete! \tAverage Loss:  0.09136901050806046\n",
      "\tEpoch 33 complete! \tAverage Loss:  0.08934957534074783\n",
      "\tEpoch 34 complete! \tAverage Loss:  0.08756603300571442\n",
      "\tEpoch 35 complete! \tAverage Loss:  0.0858730673789978\n",
      "\tEpoch 36 complete! \tAverage Loss:  0.08413700014352798\n",
      "\tEpoch 37 complete! \tAverage Loss:  0.08228662610054016\n",
      "\tEpoch 38 complete! \tAverage Loss:  0.08031029254198074\n",
      "\tEpoch 39 complete! \tAverage Loss:  0.07816465944051743\n",
      "\tEpoch 40 complete! \tAverage Loss:  0.07591214776039124\n",
      "\tEpoch 41 complete! \tAverage Loss:  0.07370158284902573\n",
      "\tEpoch 42 complete! \tAverage Loss:  0.0715627670288086\n",
      "\tEpoch 43 complete! \tAverage Loss:  0.06952497363090515\n",
      "\tEpoch 44 complete! \tAverage Loss:  0.06762344390153885\n",
      "\tEpoch 45 complete! \tAverage Loss:  0.06588651984930038\n",
      "\tEpoch 46 complete! \tAverage Loss:  0.06430089473724365\n",
      "\tEpoch 47 complete! \tAverage Loss:  0.06285665929317474\n",
      "\tEpoch 48 complete! \tAverage Loss:  0.06155397370457649\n",
      "\tEpoch 49 complete! \tAverage Loss:  0.06038907170295715\n",
      "\tEpoch 50 complete! \tAverage Loss:  0.05934051796793938\n",
      "\tEpoch 51 complete! \tAverage Loss:  0.05837757885456085\n",
      "\tEpoch 52 complete! \tAverage Loss:  0.05744863674044609\n",
      "\tEpoch 53 complete! \tAverage Loss:  0.05650194361805916\n",
      "\tEpoch 54 complete! \tAverage Loss:  0.055548980832099915\n",
      "\tEpoch 55 complete! \tAverage Loss:  0.0546269416809082\n",
      "\tEpoch 56 complete! \tAverage Loss:  0.0537407286465168\n",
      "\tEpoch 57 complete! \tAverage Loss:  0.05288710817694664\n",
      "\tEpoch 58 complete! \tAverage Loss:  0.052087120711803436\n",
      "\tEpoch 59 complete! \tAverage Loss:  0.05136629194021225\n",
      "\tEpoch 60 complete! \tAverage Loss:  0.050718653947114944\n",
      "\tEpoch 61 complete! \tAverage Loss:  0.05012473464012146\n",
      "\tEpoch 62 complete! \tAverage Loss:  0.049569569528102875\n",
      "\tEpoch 63 complete! \tAverage Loss:  0.04904166981577873\n",
      "\tEpoch 64 complete! \tAverage Loss:  0.04853019118309021\n",
      "\tEpoch 65 complete! \tAverage Loss:  0.048030681908130646\n",
      "\tEpoch 66 complete! \tAverage Loss:  0.04753924533724785\n",
      "\tEpoch 67 complete! \tAverage Loss:  0.047048669308423996\n",
      "\tEpoch 68 complete! \tAverage Loss:  0.046560924500226974\n",
      "\tEpoch 69 complete! \tAverage Loss:  0.04608328267931938\n",
      "\tEpoch 70 complete! \tAverage Loss:  0.04561576247215271\n",
      "\tEpoch 71 complete! \tAverage Loss:  0.045154694467782974\n",
      "\tEpoch 72 complete! \tAverage Loss:  0.044699814170598984\n",
      "\tEpoch 73 complete! \tAverage Loss:  0.0442507304251194\n",
      "\tEpoch 74 complete! \tAverage Loss:  0.04380011558532715\n",
      "\tEpoch 75 complete! \tAverage Loss:  0.043346017599105835\n",
      "\tEpoch 76 complete! \tAverage Loss:  0.04290146753191948\n",
      "\tEpoch 77 complete! \tAverage Loss:  0.04247300699353218\n",
      "\tEpoch 78 complete! \tAverage Loss:  0.0420646071434021\n",
      "\tEpoch 79 complete! \tAverage Loss:  0.04167972132563591\n",
      "\tEpoch 80 complete! \tAverage Loss:  0.04130683094263077\n",
      "\tEpoch 81 complete! \tAverage Loss:  0.04090617224574089\n",
      "\tEpoch 82 complete! \tAverage Loss:  0.04042736068367958\n",
      "\tEpoch 83 complete! \tAverage Loss:  0.03986011818051338\n",
      "\tEpoch 84 complete! \tAverage Loss:  0.03923315554857254\n",
      "\tEpoch 85 complete! \tAverage Loss:  0.03857312351465225\n",
      "\tEpoch 86 complete! \tAverage Loss:  0.03792079910635948\n",
      "\tEpoch 87 complete! \tAverage Loss:  0.03728247061371803\n",
      "\tEpoch 88 complete! \tAverage Loss:  0.036674223840236664\n",
      "\tEpoch 89 complete! \tAverage Loss:  0.03610968589782715\n",
      "\tEpoch 90 complete! \tAverage Loss:  0.03560585528612137\n",
      "\tEpoch 91 complete! \tAverage Loss:  0.035134218633174896\n",
      "\tEpoch 92 complete! \tAverage Loss:  0.03464308753609657\n",
      "\tEpoch 93 complete! \tAverage Loss:  0.03408404439687729\n",
      "\tEpoch 94 complete! \tAverage Loss:  0.03348718583583832\n",
      "\tEpoch 95 complete! \tAverage Loss:  0.03292838856577873\n",
      "\tEpoch 96 complete! \tAverage Loss:  0.03244810551404953\n",
      "\tEpoch 97 complete! \tAverage Loss:  0.03204165771603584\n",
      "\tEpoch 98 complete! \tAverage Loss:  0.03169257938861847\n",
      "\tEpoch 99 complete! \tAverage Loss:  0.031381092965602875\n",
      "\tEpoch 100 complete! \tAverage Loss:  0.031098751351237297\n",
      "\tEpoch 101 complete! \tAverage Loss:  0.030832866206765175\n",
      "\tEpoch 102 complete! \tAverage Loss:  0.030569786205887794\n",
      "\tEpoch 103 complete! \tAverage Loss:  0.030308732762932777\n",
      "\tEpoch 104 complete! \tAverage Loss:  0.030048642307519913\n",
      "\tEpoch 105 complete! \tAverage Loss:  0.029784277081489563\n",
      "\tEpoch 106 complete! \tAverage Loss:  0.0295150987803936\n",
      "\tEpoch 107 complete! \tAverage Loss:  0.029243620112538338\n",
      "\tEpoch 108 complete! \tAverage Loss:  0.02897622250020504\n",
      "\tEpoch 109 complete! \tAverage Loss:  0.0287293903529644\n",
      "\tEpoch 110 complete! \tAverage Loss:  0.028499973937869072\n",
      "\tEpoch 111 complete! \tAverage Loss:  0.028275443241000175\n",
      "\tEpoch 112 complete! \tAverage Loss:  0.028057344257831573\n",
      "\tEpoch 113 complete! \tAverage Loss:  0.02784830704331398\n",
      "\tEpoch 114 complete! \tAverage Loss:  0.027647914364933968\n",
      "\tEpoch 115 complete! \tAverage Loss:  0.027453536167740822\n",
      "\tEpoch 116 complete! \tAverage Loss:  0.027266571298241615\n",
      "\tEpoch 117 complete! \tAverage Loss:  0.027085935696959496\n",
      "\tEpoch 118 complete! \tAverage Loss:  0.026909681037068367\n",
      "\tEpoch 119 complete! \tAverage Loss:  0.026737967506051064\n",
      "\tEpoch 120 complete! \tAverage Loss:  0.026568975299596786\n",
      "\tEpoch 121 complete! \tAverage Loss:  0.02639777399599552\n",
      "\tEpoch 122 complete! \tAverage Loss:  0.026222534477710724\n",
      "\tEpoch 123 complete! \tAverage Loss:  0.026040757074952126\n",
      "\tEpoch 124 complete! \tAverage Loss:  0.025865787640213966\n",
      "\tEpoch 125 complete! \tAverage Loss:  0.02570362389087677\n",
      "\tEpoch 126 complete! \tAverage Loss:  0.025533093139529228\n",
      "\tEpoch 127 complete! \tAverage Loss:  0.025343474000692368\n",
      "\tEpoch 128 complete! \tAverage Loss:  0.02517472580075264\n",
      "\tEpoch 129 complete! \tAverage Loss:  0.02500992640852928\n",
      "\tEpoch 130 complete! \tAverage Loss:  0.024848228320479393\n",
      "\tEpoch 131 complete! \tAverage Loss:  0.024701040238142014\n",
      "\tEpoch 132 complete! \tAverage Loss:  0.024569151923060417\n",
      "\tEpoch 133 complete! \tAverage Loss:  0.024439122527837753\n",
      "\tEpoch 134 complete! \tAverage Loss:  0.02431364730000496\n",
      "\tEpoch 135 complete! \tAverage Loss:  0.024174505844712257\n",
      "\tEpoch 136 complete! \tAverage Loss:  0.023974671959877014\n",
      "\tEpoch 137 complete! \tAverage Loss:  0.023663906380534172\n",
      "\tEpoch 138 complete! \tAverage Loss:  0.023303179070353508\n",
      "\tEpoch 139 complete! \tAverage Loss:  0.022944267839193344\n",
      "\tEpoch 140 complete! \tAverage Loss:  0.022612985223531723\n",
      "\tEpoch 141 complete! \tAverage Loss:  0.022324634715914726\n",
      "\tEpoch 142 complete! \tAverage Loss:  0.02210111729800701\n",
      "\tEpoch 143 complete! \tAverage Loss:  0.021945958957076073\n",
      "\tEpoch 144 complete! \tAverage Loss:  0.02184474840760231\n",
      "\tEpoch 145 complete! \tAverage Loss:  0.021780474111437798\n",
      "\tEpoch 146 complete! \tAverage Loss:  0.021727364510297775\n",
      "\tEpoch 147 complete! \tAverage Loss:  0.021671950817108154\n",
      "\tEpoch 148 complete! \tAverage Loss:  0.021595796570181847\n",
      "\tEpoch 149 complete! \tAverage Loss:  0.021510014310479164\n",
      "\tEpoch 150 complete! \tAverage Loss:  0.021413654088974\n",
      "\tEpoch 151 complete! \tAverage Loss:  0.021302396431565285\n",
      "\tEpoch 152 complete! \tAverage Loss:  0.02118980698287487\n",
      "\tEpoch 153 complete! \tAverage Loss:  0.021079782396554947\n",
      "\tEpoch 154 complete! \tAverage Loss:  0.02097548544406891\n",
      "\tEpoch 155 complete! \tAverage Loss:  0.02088177390396595\n",
      "\tEpoch 156 complete! \tAverage Loss:  0.020800428465008736\n",
      "\tEpoch 157 complete! \tAverage Loss:  0.020725512877106667\n",
      "\tEpoch 158 complete! \tAverage Loss:  0.02065478265285492\n",
      "\tEpoch 159 complete! \tAverage Loss:  0.020585056394338608\n",
      "\tEpoch 160 complete! \tAverage Loss:  0.02051387168467045\n",
      "\tEpoch 161 complete! \tAverage Loss:  0.020436301827430725\n",
      "\tEpoch 162 complete! \tAverage Loss:  0.020350923761725426\n",
      "\tEpoch 163 complete! \tAverage Loss:  0.020262451842427254\n",
      "\tEpoch 164 complete! \tAverage Loss:  0.020183850079774857\n",
      "\tEpoch 165 complete! \tAverage Loss:  0.020115677267313004\n",
      "\tEpoch 166 complete! \tAverage Loss:  0.0200513806194067\n",
      "\tEpoch 167 complete! \tAverage Loss:  0.019985917955636978\n",
      "\tEpoch 168 complete! \tAverage Loss:  0.01991785503923893\n",
      "\tEpoch 169 complete! \tAverage Loss:  0.01984969899058342\n",
      "\tEpoch 170 complete! \tAverage Loss:  0.01978381536900997\n",
      "\tEpoch 171 complete! \tAverage Loss:  0.01972067728638649\n",
      "\tEpoch 172 complete! \tAverage Loss:  0.0196602214127779\n",
      "\tEpoch 173 complete! \tAverage Loss:  0.019602997228503227\n",
      "\tEpoch 174 complete! \tAverage Loss:  0.019549578428268433\n",
      "\tEpoch 175 complete! \tAverage Loss:  0.01949905790388584\n",
      "\tEpoch 176 complete! \tAverage Loss:  0.019450083374977112\n",
      "\tEpoch 177 complete! \tAverage Loss:  0.01940206065773964\n",
      "\tEpoch 178 complete! \tAverage Loss:  0.01935480162501335\n",
      "\tEpoch 179 complete! \tAverage Loss:  0.019307784736156464\n",
      "\tEpoch 180 complete! \tAverage Loss:  0.019260816276073456\n",
      "\tEpoch 181 complete! \tAverage Loss:  0.019213970750570297\n",
      "\tEpoch 182 complete! \tAverage Loss:  0.019167356193065643\n",
      "\tEpoch 183 complete! \tAverage Loss:  0.019121401011943817\n",
      "\tEpoch 184 complete! \tAverage Loss:  0.019076121971011162\n",
      "\tEpoch 185 complete! \tAverage Loss:  0.01903161220252514\n",
      "\tEpoch 186 complete! \tAverage Loss:  0.018987929448485374\n",
      "\tEpoch 187 complete! \tAverage Loss:  0.018944989889860153\n",
      "\tEpoch 188 complete! \tAverage Loss:  0.018902873620390892\n",
      "\tEpoch 189 complete! \tAverage Loss:  0.01886148750782013\n",
      "\tEpoch 190 complete! \tAverage Loss:  0.018820537254214287\n",
      "\tEpoch 191 complete! \tAverage Loss:  0.018779924139380455\n",
      "\tEpoch 192 complete! \tAverage Loss:  0.01873934082686901\n",
      "\tEpoch 193 complete! \tAverage Loss:  0.018698448315262794\n",
      "\tEpoch 194 complete! \tAverage Loss:  0.018657470121979713\n",
      "\tEpoch 195 complete! \tAverage Loss:  0.01861589401960373\n",
      "\tEpoch 196 complete! \tAverage Loss:  0.0185745507478714\n",
      "\tEpoch 197 complete! \tAverage Loss:  0.018538564443588257\n",
      "\tEpoch 198 complete! \tAverage Loss:  0.018504643812775612\n",
      "\tEpoch 199 complete! \tAverage Loss:  0.018459448590874672\n",
      "\tEpoch 200 complete! \tAverage Loss:  0.018403274938464165\n",
      "Finish!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Start training VAE...\")\n",
    "epochs = 200 #\n",
    "AE_train(dataset = input, model = model, epochs = epochs, loss_ = loss_function, DEVICE = DEVICE)\n",
    "print(\"Finish!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4pXfwO5jW3vb"
   },
   "outputs": [],
   "source": [
    "#torch.save(model, 'model_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "zs = []\n",
    "for i in input:\n",
    "    _,z= model(i)\n",
    "    zs.append(z.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted mutual info score: 0.7273187591444649\n",
      "Adjusted rand score: 0.7387883468769058\n",
      "Completeness score: 0.7175361622829975\n",
      "Fowlkes mallow score: 0.8307233329399385\n",
      "Homogeneity: 0.7380174753302193 \n",
      "Completeness score: 0.7175361622829975\n",
      "V measure: 0.7276327209962296\n",
      "Mutual information between two clusterings score: 0.7783253223297782\n",
      "Normalized Mutual Information between two clusterings score: 0.7276327209962294\n",
      "Rand score: 0.880926946946175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=\"auto\").fit(zs)\n",
    "predict = kmeans.labels_\n",
    "target = b['label']\n",
    "evaluate(target = target, predict = predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7qYAoS98n8t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
