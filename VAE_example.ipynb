{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GzC-ArZn6F7a"
   },
   "outputs": [],
   "source": [
    "# link drive https://drive.google.com/drive/folders/18-MEX-KK80-NhISMBUj1K-AsJyr-EuX_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ONNhpRO7YZ8o"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qzmwPgoXZGm2"
   },
   "outputs": [],
   "source": [
    "from Model import Encoder, Decoder, VAE_model\n",
    "from Train import VAE_train\n",
    "from LOSS import VAE_loss_function\n",
    "from sklearnEval import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lJY56oNGYsPO"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "with open('X_Y.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "input = torch.from_numpy(b['data'].astype('float'))\n",
    "test = input\n",
    "input = input.float()\n",
    "\n",
    "target = b['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GLPNvgeJY2L-"
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")\n",
    "batch_size = 1\n",
    "x_dim  = 500\n",
    "hidden_dim = 300\n",
    "latent_dim = 15\n",
    "lr = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YP5D91BLK_gv"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(input_dim=x_dim, hidden_dim=hidden_dim, latent_dim=latent_dim)\n",
    "decoder = Decoder(latent_dim=latent_dim, hidden_dim = hidden_dim, output_dim = x_dim)\n",
    "model = VAE_model(Encoder=encoder, Decoder=decoder).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zupXwYMEK_gw"
   },
   "source": [
    "### Step 3. Define Loss function (reprod. loss) and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "h9DOjmpEK_gw"
   },
   "outputs": [],
   "source": [
    "loss_function = VAE_loss_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUfzB9PUamCj"
   },
   "source": [
    "## Step 4: Train VAE **model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EDPFHwTyakDs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training VAE...\n",
      "\tEpoch 1 complete! \tAverage Loss:  0.28333547711372375\n",
      "\tEpoch 2 complete! \tAverage Loss:  0.28296416997909546\n",
      "\tEpoch 3 complete! \tAverage Loss:  0.28124022483825684\n",
      "\tEpoch 4 complete! \tAverage Loss:  0.27948686480522156\n",
      "\tEpoch 5 complete! \tAverage Loss:  0.27788829803466797\n",
      "\tEpoch 6 complete! \tAverage Loss:  0.27598893642425537\n",
      "\tEpoch 7 complete! \tAverage Loss:  0.2735947072505951\n",
      "\tEpoch 8 complete! \tAverage Loss:  0.2709426283836365\n",
      "\tEpoch 9 complete! \tAverage Loss:  0.26797497272491455\n",
      "\tEpoch 10 complete! \tAverage Loss:  0.26468905806541443\n",
      "\tEpoch 11 complete! \tAverage Loss:  0.26121440529823303\n",
      "\tEpoch 12 complete! \tAverage Loss:  0.257439523935318\n",
      "\tEpoch 13 complete! \tAverage Loss:  0.25355806946754456\n",
      "\tEpoch 14 complete! \tAverage Loss:  0.24938252568244934\n",
      "\tEpoch 15 complete! \tAverage Loss:  0.24507392942905426\n",
      "\tEpoch 16 complete! \tAverage Loss:  0.2406587451696396\n",
      "\tEpoch 17 complete! \tAverage Loss:  0.23616936802864075\n",
      "\tEpoch 18 complete! \tAverage Loss:  0.2316216230392456\n",
      "\tEpoch 19 complete! \tAverage Loss:  0.2271471619606018\n",
      "\tEpoch 20 complete! \tAverage Loss:  0.2226797342300415\n",
      "\tEpoch 21 complete! \tAverage Loss:  0.2180950790643692\n",
      "\tEpoch 22 complete! \tAverage Loss:  0.21358583867549896\n",
      "\tEpoch 23 complete! \tAverage Loss:  0.2091256082057953\n",
      "\tEpoch 24 complete! \tAverage Loss:  0.2047342211008072\n",
      "\tEpoch 25 complete! \tAverage Loss:  0.20053787529468536\n",
      "\tEpoch 26 complete! \tAverage Loss:  0.1960560828447342\n",
      "\tEpoch 27 complete! \tAverage Loss:  0.1917840838432312\n",
      "\tEpoch 28 complete! \tAverage Loss:  0.18750795722007751\n",
      "\tEpoch 29 complete! \tAverage Loss:  0.18338517844676971\n",
      "\tEpoch 30 complete! \tAverage Loss:  0.17934542894363403\n",
      "\tEpoch 31 complete! \tAverage Loss:  0.17527011036872864\n",
      "\tEpoch 32 complete! \tAverage Loss:  0.17139391601085663\n",
      "\tEpoch 33 complete! \tAverage Loss:  0.16736751794815063\n",
      "\tEpoch 34 complete! \tAverage Loss:  0.16360844671726227\n",
      "\tEpoch 35 complete! \tAverage Loss:  0.15979346632957458\n",
      "\tEpoch 36 complete! \tAverage Loss:  0.15619179606437683\n",
      "\tEpoch 37 complete! \tAverage Loss:  0.15260180830955505\n",
      "\tEpoch 38 complete! \tAverage Loss:  0.14902746677398682\n",
      "\tEpoch 39 complete! \tAverage Loss:  0.14557209610939026\n",
      "\tEpoch 40 complete! \tAverage Loss:  0.14227347075939178\n",
      "\tEpoch 41 complete! \tAverage Loss:  0.13899646699428558\n",
      "\tEpoch 42 complete! \tAverage Loss:  0.13572733104228973\n",
      "\tEpoch 43 complete! \tAverage Loss:  0.13271908462047577\n",
      "\tEpoch 44 complete! \tAverage Loss:  0.12958155572414398\n",
      "\tEpoch 45 complete! \tAverage Loss:  0.12668578326702118\n",
      "\tEpoch 46 complete! \tAverage Loss:  0.12390021979808807\n",
      "\tEpoch 47 complete! \tAverage Loss:  0.12105649709701538\n",
      "\tEpoch 48 complete! \tAverage Loss:  0.11848320066928864\n",
      "\tEpoch 49 complete! \tAverage Loss:  0.11595167219638824\n",
      "\tEpoch 50 complete! \tAverage Loss:  0.11356688290834427\n",
      "\tEpoch 51 complete! \tAverage Loss:  0.11119087785482407\n",
      "\tEpoch 52 complete! \tAverage Loss:  0.10879767686128616\n",
      "\tEpoch 53 complete! \tAverage Loss:  0.10660866647958755\n",
      "\tEpoch 54 complete! \tAverage Loss:  0.1046263724565506\n",
      "\tEpoch 55 complete! \tAverage Loss:  0.10249502211809158\n",
      "\tEpoch 56 complete! \tAverage Loss:  0.10048416256904602\n",
      "\tEpoch 57 complete! \tAverage Loss:  0.09864768385887146\n",
      "\tEpoch 58 complete! \tAverage Loss:  0.09681911766529083\n",
      "\tEpoch 59 complete! \tAverage Loss:  0.0950617790222168\n",
      "\tEpoch 60 complete! \tAverage Loss:  0.09335820376873016\n",
      "\tEpoch 61 complete! \tAverage Loss:  0.0917443335056305\n",
      "\tEpoch 62 complete! \tAverage Loss:  0.09033803641796112\n",
      "\tEpoch 63 complete! \tAverage Loss:  0.0887773260474205\n",
      "\tEpoch 64 complete! \tAverage Loss:  0.08726705610752106\n",
      "\tEpoch 65 complete! \tAverage Loss:  0.085921511054039\n",
      "\tEpoch 66 complete! \tAverage Loss:  0.08468929678201675\n",
      "\tEpoch 67 complete! \tAverage Loss:  0.08342959731817245\n",
      "\tEpoch 68 complete! \tAverage Loss:  0.08220279961824417\n",
      "\tEpoch 69 complete! \tAverage Loss:  0.08096159994602203\n",
      "\tEpoch 70 complete! \tAverage Loss:  0.07977112382650375\n",
      "\tEpoch 71 complete! \tAverage Loss:  0.07866856455802917\n",
      "\tEpoch 72 complete! \tAverage Loss:  0.07762588560581207\n",
      "\tEpoch 73 complete! \tAverage Loss:  0.07668140530586243\n",
      "\tEpoch 74 complete! \tAverage Loss:  0.07568664103746414\n",
      "\tEpoch 75 complete! \tAverage Loss:  0.07479801028966904\n",
      "\tEpoch 76 complete! \tAverage Loss:  0.07392449676990509\n",
      "\tEpoch 77 complete! \tAverage Loss:  0.07305655628442764\n",
      "\tEpoch 78 complete! \tAverage Loss:  0.0720757469534874\n",
      "\tEpoch 79 complete! \tAverage Loss:  0.07114017009735107\n",
      "\tEpoch 80 complete! \tAverage Loss:  0.07036181539297104\n",
      "\tEpoch 81 complete! \tAverage Loss:  0.0695873349905014\n",
      "\tEpoch 82 complete! \tAverage Loss:  0.068974070250988\n",
      "\tEpoch 83 complete! \tAverage Loss:  0.06824865192174911\n",
      "\tEpoch 84 complete! \tAverage Loss:  0.067501500248909\n",
      "\tEpoch 85 complete! \tAverage Loss:  0.06671217828989029\n",
      "\tEpoch 86 complete! \tAverage Loss:  0.06600561738014221\n",
      "\tEpoch 87 complete! \tAverage Loss:  0.06548411399126053\n",
      "\tEpoch 88 complete! \tAverage Loss:  0.0649857446551323\n",
      "\tEpoch 89 complete! \tAverage Loss:  0.06437085568904877\n",
      "\tEpoch 90 complete! \tAverage Loss:  0.0636422261595726\n",
      "\tEpoch 91 complete! \tAverage Loss:  0.0630405992269516\n",
      "\tEpoch 92 complete! \tAverage Loss:  0.06249454244971275\n",
      "\tEpoch 93 complete! \tAverage Loss:  0.06199600175023079\n",
      "\tEpoch 94 complete! \tAverage Loss:  0.06147164851427078\n",
      "\tEpoch 95 complete! \tAverage Loss:  0.06089869886636734\n",
      "\tEpoch 96 complete! \tAverage Loss:  0.060400597751140594\n",
      "\tEpoch 97 complete! \tAverage Loss:  0.05992516502737999\n",
      "\tEpoch 98 complete! \tAverage Loss:  0.05939097702503204\n",
      "\tEpoch 99 complete! \tAverage Loss:  0.05884794518351555\n",
      "\tEpoch 100 complete! \tAverage Loss:  0.05840013921260834\n",
      "\tEpoch 101 complete! \tAverage Loss:  0.058013927191495895\n",
      "\tEpoch 102 complete! \tAverage Loss:  0.057655785232782364\n",
      "\tEpoch 103 complete! \tAverage Loss:  0.05724063515663147\n",
      "\tEpoch 104 complete! \tAverage Loss:  0.056835416704416275\n",
      "\tEpoch 105 complete! \tAverage Loss:  0.05639416724443436\n",
      "\tEpoch 106 complete! \tAverage Loss:  0.05611695721745491\n",
      "\tEpoch 107 complete! \tAverage Loss:  0.05571006238460541\n",
      "\tEpoch 108 complete! \tAverage Loss:  0.05529886484146118\n",
      "\tEpoch 109 complete! \tAverage Loss:  0.05490561202168465\n",
      "\tEpoch 110 complete! \tAverage Loss:  0.05448226258158684\n",
      "\tEpoch 111 complete! \tAverage Loss:  0.05414050817489624\n",
      "\tEpoch 112 complete! \tAverage Loss:  0.0537700355052948\n",
      "\tEpoch 113 complete! \tAverage Loss:  0.053285326808691025\n",
      "\tEpoch 114 complete! \tAverage Loss:  0.05296158790588379\n",
      "\tEpoch 115 complete! \tAverage Loss:  0.05270657688379288\n",
      "\tEpoch 116 complete! \tAverage Loss:  0.05248575657606125\n",
      "\tEpoch 117 complete! \tAverage Loss:  0.0522317998111248\n",
      "\tEpoch 118 complete! \tAverage Loss:  0.052001338452100754\n",
      "\tEpoch 119 complete! \tAverage Loss:  0.051699694246053696\n",
      "\tEpoch 120 complete! \tAverage Loss:  0.051480334252119064\n",
      "\tEpoch 121 complete! \tAverage Loss:  0.05097445473074913\n",
      "\tEpoch 122 complete! \tAverage Loss:  0.050541747361421585\n",
      "\tEpoch 123 complete! \tAverage Loss:  0.050185106694698334\n",
      "\tEpoch 124 complete! \tAverage Loss:  0.05000259354710579\n",
      "\tEpoch 125 complete! \tAverage Loss:  0.04973594471812248\n",
      "\tEpoch 126 complete! \tAverage Loss:  0.0494338795542717\n",
      "\tEpoch 127 complete! \tAverage Loss:  0.04914979264140129\n",
      "\tEpoch 128 complete! \tAverage Loss:  0.048918865621089935\n",
      "\tEpoch 129 complete! \tAverage Loss:  0.04858854413032532\n",
      "\tEpoch 130 complete! \tAverage Loss:  0.048303503543138504\n",
      "\tEpoch 131 complete! \tAverage Loss:  0.04814234375953674\n",
      "\tEpoch 132 complete! \tAverage Loss:  0.04797330126166344\n",
      "\tEpoch 133 complete! \tAverage Loss:  0.04781804233789444\n",
      "\tEpoch 134 complete! \tAverage Loss:  0.047612790018320084\n",
      "\tEpoch 135 complete! \tAverage Loss:  0.04733751714229584\n",
      "\tEpoch 136 complete! \tAverage Loss:  0.04697462171316147\n",
      "\tEpoch 137 complete! \tAverage Loss:  0.04665898159146309\n",
      "\tEpoch 138 complete! \tAverage Loss:  0.04647555574774742\n",
      "\tEpoch 139 complete! \tAverage Loss:  0.046520307660102844\n",
      "\tEpoch 140 complete! \tAverage Loss:  0.046397168189287186\n",
      "\tEpoch 141 complete! \tAverage Loss:  0.04617365077137947\n",
      "\tEpoch 142 complete! \tAverage Loss:  0.04574517533183098\n",
      "\tEpoch 143 complete! \tAverage Loss:  0.045382626354694366\n",
      "\tEpoch 144 complete! \tAverage Loss:  0.04522073268890381\n",
      "\tEpoch 145 complete! \tAverage Loss:  0.04514883831143379\n",
      "\tEpoch 146 complete! \tAverage Loss:  0.04511832445859909\n",
      "\tEpoch 147 complete! \tAverage Loss:  0.044825032353401184\n",
      "\tEpoch 148 complete! \tAverage Loss:  0.044402964413166046\n",
      "\tEpoch 149 complete! \tAverage Loss:  0.04415237158536911\n",
      "\tEpoch 150 complete! \tAverage Loss:  0.04412591829895973\n",
      "\tEpoch 151 complete! \tAverage Loss:  0.04409791901707649\n",
      "\tEpoch 152 complete! \tAverage Loss:  0.04379522427916527\n",
      "\tEpoch 153 complete! \tAverage Loss:  0.043450672179460526\n",
      "\tEpoch 154 complete! \tAverage Loss:  0.043285056948661804\n",
      "\tEpoch 155 complete! \tAverage Loss:  0.04319881275296211\n",
      "\tEpoch 156 complete! \tAverage Loss:  0.043057166039943695\n",
      "\tEpoch 157 complete! \tAverage Loss:  0.04276936501264572\n",
      "\tEpoch 158 complete! \tAverage Loss:  0.042642537504434586\n",
      "\tEpoch 159 complete! \tAverage Loss:  0.04246533662080765\n",
      "\tEpoch 160 complete! \tAverage Loss:  0.042236678302288055\n",
      "\tEpoch 161 complete! \tAverage Loss:  0.042003292590379715\n",
      "\tEpoch 162 complete! \tAverage Loss:  0.041967395693063736\n",
      "\tEpoch 163 complete! \tAverage Loss:  0.04185706377029419\n",
      "\tEpoch 164 complete! \tAverage Loss:  0.041634056717157364\n",
      "\tEpoch 165 complete! \tAverage Loss:  0.041342124342918396\n",
      "\tEpoch 166 complete! \tAverage Loss:  0.041218001395463943\n",
      "\tEpoch 167 complete! \tAverage Loss:  0.04116826876997948\n",
      "\tEpoch 168 complete! \tAverage Loss:  0.041025083512067795\n",
      "\tEpoch 169 complete! \tAverage Loss:  0.040815163403749466\n",
      "\tEpoch 170 complete! \tAverage Loss:  0.04076516628265381\n",
      "\tEpoch 171 complete! \tAverage Loss:  0.04072370007634163\n",
      "\tEpoch 172 complete! \tAverage Loss:  0.04050156846642494\n",
      "\tEpoch 173 complete! \tAverage Loss:  0.04022729769349098\n",
      "\tEpoch 174 complete! \tAverage Loss:  0.04008878767490387\n",
      "\tEpoch 175 complete! \tAverage Loss:  0.03996078670024872\n",
      "\tEpoch 176 complete! \tAverage Loss:  0.03972069174051285\n",
      "\tEpoch 177 complete! \tAverage Loss:  0.0395272932946682\n",
      "\tEpoch 178 complete! \tAverage Loss:  0.03949305787682533\n",
      "\tEpoch 179 complete! \tAverage Loss:  0.03940277174115181\n",
      "\tEpoch 180 complete! \tAverage Loss:  0.03921549767255783\n",
      "\tEpoch 181 complete! \tAverage Loss:  0.0390864796936512\n",
      "\tEpoch 182 complete! \tAverage Loss:  0.039041873067617416\n",
      "\tEpoch 183 complete! \tAverage Loss:  0.039006415754556656\n",
      "\tEpoch 184 complete! \tAverage Loss:  0.03884278982877731\n",
      "\tEpoch 185 complete! \tAverage Loss:  0.03869040310382843\n",
      "\tEpoch 186 complete! \tAverage Loss:  0.03859143331646919\n",
      "\tEpoch 187 complete! \tAverage Loss:  0.038442786782979965\n",
      "\tEpoch 188 complete! \tAverage Loss:  0.03819568455219269\n",
      "\tEpoch 189 complete! \tAverage Loss:  0.03805394098162651\n",
      "\tEpoch 190 complete! \tAverage Loss:  0.03792548179626465\n",
      "\tEpoch 191 complete! \tAverage Loss:  0.037788596004247665\n",
      "\tEpoch 192 complete! \tAverage Loss:  0.037567127496004105\n",
      "\tEpoch 193 complete! \tAverage Loss:  0.03741919621825218\n",
      "\tEpoch 194 complete! \tAverage Loss:  0.03734178841114044\n",
      "\tEpoch 195 complete! \tAverage Loss:  0.037250738590955734\n",
      "\tEpoch 196 complete! \tAverage Loss:  0.03711584955453873\n",
      "\tEpoch 197 complete! \tAverage Loss:  0.03692879155278206\n",
      "\tEpoch 198 complete! \tAverage Loss:  0.03681853041052818\n",
      "\tEpoch 199 complete! \tAverage Loss:  0.03671008720993996\n",
      "\tEpoch 200 complete! \tAverage Loss:  0.036616094410419464\n",
      "Finish!!\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "VAE_train(dataset=input, model=model, epochs = epochs, loss_=loss_function, DEVICE=DEVICE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "4pXfwO5jW3vb"
   },
   "outputs": [],
   "source": [
    "#torch.save(model, 'model_2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "zs = []\n",
    "for i in input:\n",
    "    _,z,_= model(i)\n",
    "    zs.append(z.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted mutual info score: 0.7164904360892815\n",
      "Adjusted rand score: 0.6528725959368199\n",
      "Completeness score: 0.7240059245138966\n",
      "Fowlkes mallow score: 0.7807572272796492\n",
      "Homogeneity: 0.7097849052396215 \n",
      "Completeness score: 0.7240059245138966\n",
      "V measure: 0.7168248894608467\n",
      "Mutual information between two clusterings score: 0.7485507913051158\n",
      "Normalized Mutual Information between two clusterings score: 0.7168248894608467\n",
      "Rand score: 0.8384790485888879\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=\"auto\").fit(zs)\n",
    "predict = kmeans.labels_\n",
    "target = b['label']\n",
    "evaluate(target = target, predict = predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
